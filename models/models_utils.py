from custom_types import *
from abc import ABC
import math


def torch_no_grad(func):
    def wrapper(*args, **kwargs):
        with torch.no_grad():
            result = func(*args, **kwargs)
        return result

    return wrapper


class Model(nn.Module, ABC):

    def __init__(self):
        super(Model, self).__init__()
        self.save_model: Union[None, Callable[[nn.Module]]] = None

    def save(self, **kwargs):
        self.save_model(self, **kwargs)


def weights_init(m):
    classname = m.__class__.__name__
    if isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight, gain=np.sqrt(2.0))
    elif classname.find('Conv') != -1:
        nn.init.xavier_normal_(m.weight, gain=np.sqrt(2.0))
    elif classname.find('Linear') != -1:
        nn.init.xavier_normal_(m.weight, gain=np.sqrt(2.0))
    elif classname.find('Embe') != -1:
        # nn.init.xavier_uniform(m.weight, gain=np.sqrt(2.0))
        nn.init.normal_(m.weight, mean=0, std=1)


class Concatenate(nn.Module):
    def __init__(self, dim):
        super(Concatenate, self).__init__()
        self.dim = dim

    def forward(self, x):
        return torch.cat(x, dim=self.dim)


class View(nn.Module):

    def __init__(self, *shape):
        super(View, self).__init__()
        self.shape = shape

    def forward(self, x):
        return x.view(*self.shape)


class Transpose(nn.Module):

    def __init__(self, dim0, dim1):
        super(Transpose, self).__init__()
        self.dim0, self.dim1 = dim0, dim1

    def forward(self, x):
        return x.transpose(self.dim0, self.dim1)


class Dummy(nn.Module):

    def __init__(self, *args):
        super(Dummy, self).__init__()

    def forward(self, *args):
        return args[0]


class MLP(nn.Module):

    def __init__(self, ch: tuple, norm_class: Optional[nn.Module] = nn.LayerNorm, dropout=0, skip=False):
        super(MLP, self).__init__()
        layers = []
        for i in range(len(ch) - 1):
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            layers.append(nn.Linear(ch[i], ch[i + 1]))
            if i < len(ch) - 2:
                if norm_class is not None:
                    layers.append(norm_class(ch[i + 1]))
                layers.append(nn.ReLU(True))
        self.skip = skip
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        out = self.net(x)
        if self.skip:
            out = x + out
        return out


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        raise ValueError("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                         "The distribution of values may be incorrect.")

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):

    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class GMAttend(nn.Module):

    def __init__(self, hidden_dim: int):
        super(GMAttend, self).__init__()
        self.key_dim = hidden_dim // 8
        self.query_w = nn.Linear(hidden_dim, self.key_dim)
        self.key_w = nn.Linear(hidden_dim, self.key_dim)
        self.value_w = nn.Linear(hidden_dim, hidden_dim)
        self.softmax = nn.Softmax(dim=3)
        self.gamma = nn.Parameter(torch.zeros(1))
        self.scale = 1 / torch.sqrt(torch.tensor(self.key_dim, dtype=torch.float32))

    def forward(self, x):
        queries = self.query_w(x)
        keys = self.key_w(x)
        vals = self.value_w(x)
        attention = self.softmax(torch.einsum('bgqf,bgkf->bgqk', queries, keys))
        out = torch.einsum('bgvf,bgqv->bgqf', vals, attention)
        out = self.gamma * out + x
        return out


def dkl(mu, log_sigma):
    if log_sigma is None:
        return torch.zeros(1).to(mu.device)
    else:
        return 0.5 * torch.sum(torch.exp(log_sigma) - 1 - log_sigma + mu ** 2) / (mu.shape[0] * mu.shape[1])


def recursive_to(item, device):
    if type(item) is T:
        return item.to(device)
    elif type(item) is tuple or type(item) is list:
        return [recursive_to(item[i], device) for i in range(len(item))]
    return item

###------------------------- Public repository version -------------------------###

# from custom_types import *
# from abc import ABC
# import math


# def torch_no_grad(func):
#     def wrapper(*args, **kwargs):
#         with torch.no_grad():
#             result = func(*args, **kwargs)
#         return result

#     return wrapper


# class Model(nn.Module, ABC):

#     def __init__(self):
#         super(Model, self).__init__()
#         self.save_model: Union[None, Callable[[nn.Module]]] = None

#     def save(self, **kwargs):
#         self.save_model(self, **kwargs)


# class Concatenate(nn.Module):
#     def __init__(self, dim):
#         super(Concatenate, self).__init__()
#         self.dim = dim

#     def forward(self, x):
#         return torch.cat(x, dim=self.dim)


# class View(nn.Module):

#     def __init__(self, *shape):
#         super(View, self).__init__()
#         self.shape = shape

#     def forward(self, x):
#         return x.view(*self.shape)


# class Transpose(nn.Module):

#     def __init__(self, dim0, dim1):
#         super(Transpose, self).__init__()
#         self.dim0, self.dim1 = dim0, dim1

#     def forward(self, x):
#         return x.transpose(self.dim0, self.dim1)


# class Dummy(nn.Module):

#     def __init__(self, *args):
#         super(Dummy, self).__init__()

#     def forward(self, *args):
#         return args[0]


# class SineLayer(nn.Module):
#     """
#     From the siren repository
#     https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb
#     """
#     def __init__(self, in_features, out_features, bias=True,
#                  is_first=False, omega_0=30):
#         super().__init__()
#         self.omega_0 = omega_0
#         self.is_first = is_first

#         self.in_features = in_features
#         self.linear = nn.Linear(in_features, out_features, bias=bias)
#         self.output_channels = out_features
#         self.init_weights()

#     def init_weights(self):
#         with torch.no_grad():
#             if self.is_first:
#                 self.linear.weight.uniform_(-1 / self.in_features,
#                                             1 / self.in_features)
#             else:
#                 self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,
#                                             np.sqrt(6 / self.in_features) / self.omega_0)

#     def forward(self, input):
#         return torch.sin(self.omega_0 * self.linear(input))


# class MLP(nn.Module):

#     def forward(self, x, *_):
#         return self.net(x)

#     def __init__(self, ch: Union[List[int], Tuple[int, ...]], act: nn.Module = nn.ReLU,
#                  weight_norm=False):
#         super(MLP, self).__init__()
#         layers = []
#         for i in range(len(ch) - 1):
#             layers.append(nn.Linear(ch[i], ch[i + 1]))
#             if weight_norm:
#                 layers[-1] = nn.utils.weight_norm(layers[-1])
#             if i < len(ch) - 2:
#                 layers.append(act(True))
#         self.net = nn.Sequential(*layers)


# class GMAttend(nn.Module):

#     def __init__(self, hidden_dim: int):
#         super(GMAttend, self).__init__()
#         self.key_dim = hidden_dim // 8
#         self.query_w = nn.Linear(hidden_dim, self.key_dim)
#         self.key_w = nn.Linear(hidden_dim, self.key_dim)
#         self.value_w = nn.Linear(hidden_dim, hidden_dim)
#         self.softmax = nn.Softmax(dim=3)
#         self.gamma = nn.Parameter(torch.zeros(1))
#         self.scale = 1 / torch.sqrt(torch.tensor(self.key_dim, dtype=torch.float32))

#     def forward(self, x):
#         queries = self.query_w(x)
#         keys = self.key_w(x)
#         vals = self.value_w(x)
#         attention = self.softmax(torch.einsum('bgqf,bgkf->bgqk', queries, keys))
#         out = torch.einsum('bgvf,bgqv->bgqf', vals, attention)
#         out = self.gamma * out + x
#         return out


# def recursive_to(item, device):
#     if type(item) is T:
#         return item.to(device)
#     elif type(item) is tuple or type(item) is list:
#         return [recursive_to(item[i], device) for i in range(len(item))]
#     return item

